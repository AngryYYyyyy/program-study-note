# 搜索模块

## 一、需求分析

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/d5147546627441049ab19ac23612b8f2.png)

策略模块在发送短信失败之后，需要发送失败的短信内容和一些标识全部存储到Elasticsearch中。

咱们的烽火云短信平台，整体的设计是基于一年12亿条左右设计的。传统的MySQL，Oracle这种关系型数据库是无法满足咱们的需求的。这里直接采用Elasticsearch作为存储短信发送信息的媒介。

Elasticsearch天生分布式，存储能力肯定是木有问题。其次Elasticsearch内部提供了非常丰富的聚合函数，可以统计海量数据，方面在前端生成图表给客户做展示。

因为数据量比较大，不能把所有的数据都扔到一个索引里，这里咱们采用一年构建一个索引。用来存储短信发送的信息。

索引确认之后，主从分片也需要考虑。当然咱们的业务属于写多读少的情况。需要设置多一些主分片的数量，比如每一个主分片，设置存储100G左右的数据。因为1年大概是12亿条左右，按照1条1k算，一年大概1.12TB左右的数据。15个左右的主分片，就可以存储下全部的数据。因为读操作比较少，从分片只分配1个即可。第一点是为了承载查询压力，另外就是避免出现节点故障。而且从分片没有事，可以动态扩展。

## 二、索引准备

需要构建Elasticsearch中的索引信息，索引中存储的内容就是StandardSubmit对象中的全部信息。

构建索引时，确保Elasticsearch服务是没有问题的，在Kibana中构建指定的索引信息，语法可以基于Elasticsearch的官方文档查看：

https://www.elastic.co/guide/en/elasticsearch/reference/7.6/documents-indices.html

```json
PUT /sms_submit_log
{
    "settings" : {
      "number_of_shards" : 15, 
      "number_of_replicas" : 1
    },
    "mappings": {
      "properties": {
        "clientId": {
          "type": "long"
        },
        "ip": {
          "type": "ip"
        },
        "uid": {
          "type": "keyword"
        },
        "mobile": {
          "type": "text"
        },
        "sign": {
          "type": "keyword"
        },
        "text": {
          "type": "text",
          "analyzer": "standard"
        },
        "sendTime": {
          "type":   "date",
          "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
        },
        "fee": {
          "type": "long"
        },
        "operatorId": {
          "type": "integer"
        },
        "areaCode": {
          "type": "integer"
        },
        "area": {
          "type": "text"
        },
        "srcNumber": {
          "type": "text"
        },
        "channelId": {
          "type": "long"
        },
        "reportState": {
          "type": "integer"
        },
        "errorMsg": {
          "type": "text"
        },
        "realIP": {
          "type": "ip"
        },
        "apikey": {
          "type": "text"
        },
        "state": {
          "type": "integer"
        },
        "signId": {
          "type": "long"
        },
        "isTransfer": {
          "type": "boolean"
        },
        "oneHourLimitMilli": {
          "type": "date",
          "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
        }
  
      }
    }
}
```

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/d3201650e7b3401ba56618373e097569.png)

在安装好IK分词器后，将text短信内容，采用IK分词器进行分词，查询体验会更好

**安装IK分词器的过程：**

1、直接基于命令的形式让ES自行安装IK分词器，安装好之后只需要重新启动即可

```sh
./elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.2/elasticsearch-analysis-ik-7.6.2.zip
```

下载并安装好后，记得重启elasticsearch容器

2、因为上面下载IK分词器是从github上去下，速度有点慢，可以基于提供好的ik分词器的压缩包去安装

先找到IK分词器的zip压缩包![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/4b3186ef62c84499b65858884b067f83.png)

将压缩包解压，并修改目录的名称

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/5d11506a7e3444a280cc6dffed44e989.png)![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/6202b02e0fcc47a9baa8320ac854140b.png)

将目录全部复制到elasticsearch的容器中的指定目录下

```sh
docker cp analysis-ik/ elasticsearch:/usr/share/elasticsearch/plugins
```

确定容器内部中包含这个目录并且内部的文件也都ok，直接重启elasticsearch容器

```sh
docker restart elasticsearch
```

可以查看日志确认安装是否成功

```sh
docker logs elasticsearch | grep analysis-ik
```

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/a10f432af5ff4f0db42a5f5f7fb3676c.png)

也可以通过Kibana查询来确认IK分词器的安装情况

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/748801de6f824e34bebda922df8c6228.png)

重新构建索引，并且将text短信内容的类型的分词器设置为ik_max_word即可

```java
PUT /sms_submit_log_2023
{
  "settings" : {
    "number_of_shards" : 15, 
    "number_of_replicas" : 1
  },
  "mappings": {
    "properties": {
      "clientId": {
        "type": "long"
      },
      "ip": {
        "type": "ip"
      },
      "uid": {
        "type": "keyword"
      },
      "mobile": {
        "type": "text"
      },
      "sign": {
        "type": "keyword"
      },
      "text": {
        "type": "text",
        "analyzer": "ik_max_word"
      },
      "sendTime": {
        "type":   "date",
        "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
      },
      "fee": {
        "type": "long"
      },
      "operatorId": {
        "type": "integer"
      },
      "areaCode": {
        "type": "integer"
      },
      "area": {
        "type": "text"
      },
      "srcNumber": {
        "type": "text"
      },
      "channelId": {
        "type": "long"
      },
      "reportState": {
        "type": "integer"
      },
      "errorMsg": {
        "type": "text"
      },
      "realIP": {
        "type": "ip"
      },
      "apikey": {
        "type": "text"
      },
      "state": {
        "type": "integer"
      },
      "signId": {
        "type": "long"
      },
      "isTransfer": {
        "type": "boolean"
      },
      "oneHourLimitMilli": {
        "type": "date",
        "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
      }
  
    }
  }
}
```

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/17ee5ef7b9c041619aa4f03cdab18b6a.png)

## 三、搭建搜索模块

创建项目：……

导入依赖：

```xml
<dependencies>
    <!--        start-web-->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <!--        nacos-dis-->
    <dependency>
        <groupId>com.alibaba.cloud</groupId>
        <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
    </dependency>
    <!--        nacos-config-->
    <dependency>
        <groupId>com.alibaba.cloud</groupId>
        <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
    </dependency>
    <!--        公共组件，common-->
    <dependency>
        <groupId>com.mashibing</groupId>
        <artifactId>beacon-common</artifactId>
        <version>1.0-SNAPSHOT</version>
    </dependency>
    <!--        RabbitMQ依赖-->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-amqp</artifactId>
    </dependency>
    <!--        elasticsearch的客户端依赖-->
    <dependency>
        <groupId>org.elasticsearch.client</groupId>
        <artifactId>elasticsearch-rest-high-level-client</artifactId>
        <version>7.6.2</version>
    </dependency>
    <dependency>
        <groupId>org.elasticsearch</groupId>
        <artifactId>elasticsearch</artifactId>
        <version>7.6.2</version>
    </dependency>
</dependencies>
```

启动类：

```java
@SpringBootApplication
@EnableDiscoveryClient
public class SearchStarterApp {

    public static void main(String[] args) {
        SpringApplication.run(SearchStarterApp.class,args);
    }
}
```

配置信息：

bootstrap.yml配置

```yml
# 服务名称
spring:
  application:
    name: beacon-search
  # 多环境
  profiles:
    active: dev
  # nacos注册中心地址
  cloud:
    nacos:
      discovery:
        server-addr: 114.116.226.76:8848
      # nacos配置中心地址:
      config:
        server-addr: 114.116.226.76:8848
        file-extension: yml
        # beacon-search-dev.yml
```

nacos配置中心

```yml
# 端口号
server:
  port: 10101

spring: 
# rabbitMQ连接信息
  rabbitmq:
    host: 114.116.226.76
    port: 5672
    username: root
    password: ZhengJinWei123!
    virtual-host: /  
    listener:
      simple:
        # 开启手动ack
        acknowledge-mode: manual   
```

## 四、配置Elasticsearch连接

将连接Elasticsearch的ip和port的信息，全部封装到Nacos的配置文件中

```yml
# Elasicsearch的连接信息
elasticsearch:
  # Elasicsearch的ip和port，多个用‘,’隔间。
  hostAndPorts: 114.116.226.76:9200,114.116.226.77:9200
```

在Java中构建RestHighLevelClient

```java
@Configuration
public class RestHighLevelClientConfig {

    @Value("#{'${elasticsearch.hostAndPorts}'.split(',')}")
    private List<String> hostAndPorts;


    @Bean
    public RestHighLevelClient restHighLevelClient(){
        // 初始化连接ES的HttpHost信息
        HttpHost[] httpHosts = new HttpHost[hostAndPorts.size()];
        for (int i = 0; i < hostAndPorts.size(); i++) {
            String[] hostAndPort = hostAndPorts.get(i).split(":");
            httpHosts[i] = new HttpHost(hostAndPort[0],Integer.parseInt(hostAndPort[1]));
        }

        RestClientBuilder restClientBuilder = RestClient.builder(httpHosts);

        // 构建连接ES的client对象
        RestHighLevelClient restHighLevelClient = new RestHighLevelClient(restClientBuilder);

        // 返回
        return restHighLevelClient;
    }
}
```

## 五、构建接收写日志消费者

正常的创建消费者，消费写日志队列中的消息

```java
@Component
@Slf4j
public class SmsWriteLogListener {

    @RabbitListener(queues = RabbitMQConstants.SMS_WRITE_LOG)
    public void consume(StandardSubmit submit, Channel channel, Message message) throws IOException {
        //1、调用搜索模块的添加方法，完成添加操作
        log.info("接收到存储日志的信息   submit = {}",submit);

        //2、手动ack
        channel.basicAck(message.getMessageProperties().getDeliveryTag(),false);
    }

}
```

启动项目，查看写日志的信息

## 六、实现Elasticseach添加

实现添加就是模板代码

```java
@Service
@Slf4j
public class ElasticsearchServiceImpl implements SearchService {
    /**
     * 添加成功的result
     */
    private final String CREATED = "created";

    @Autowired
    private RestHighLevelClient restHighLevelClient;

    @Override
    public void index(String index, String id, String json) throws IOException {
        //1、构建插入数据的Request
        IndexRequest request = new IndexRequest();

        //2、给request对象封装索引信息，文档id，以及文档内容
        request.index(index);
        request.id(id);
        request.source(json, XContentType.JSON);

        //3、将request信息发送给ES服务
        IndexResponse response = restHighLevelClient.index(request, RequestOptions.DEFAULT);

        //4、校验添加是否成功
        String result = response.getResult().getLowercase();
        if(!CREATED.equals(result)){
            // 添加失败！！
            log.error("【搜索模块-写入数据失败】 index = {},id = {},json = {},result = {}",index,id,json,result);
            throw new SearchException(ExceptionEnums.SEARCH_INDEX_ERROR);
        }
        log.info("【搜索模块-写入数据成功】 索引添加成功index = {},id = {},json = {},result = {}",index,id,json,result);
    }
}
```

测试时，出现401问题，因为Elasticsearch安装时，指定了密码，修改配置信息

```java
@Configuration
public class RestHighLevelClientConfig {

    @Value("#{'${elasticsearch.hostAndPorts}'.split(',')}")
    private List<String> hostAndPorts;

    @Value("${elasticsearch.username:elastic}")
    private String username;


    @Value("${elasticsearch.password}")
    private String password;

    @Bean
    public RestHighLevelClient restHighLevelClient(){
        // 初始化连接ES的HttpHost信息
        HttpHost[] httpHosts = new HttpHost[hostAndPorts.size()];
        for (int i = 0; i < hostAndPorts.size(); i++) {
            String[] hostAndPort = hostAndPorts.get(i).split(":");
            httpHosts[i] = new HttpHost(hostAndPort[0],Integer.parseInt(hostAndPort[1]));
        }

        // 设置认证信息
        CredentialsProvider credentialsProvider = new BasicCredentialsProvider();
        credentialsProvider.setCredentials(AuthScope.ANY,new UsernamePasswordCredentials(username,password));

        // 构建时设置连接信息，基于set设置认证信息
        RestClientBuilder restClientBuilder = RestClient.builder(httpHosts);
        restClientBuilder.setHttpClientConfigCallback(f -> f.setDefaultCredentialsProvider(credentialsProvider));

        // 构建连接ES的client对象
        RestHighLevelClient restHighLevelClient = new RestHighLevelClient(restClientBuilder);

        // 返回
        return restHighLevelClient;
    }
}
```

## 七、整体测试接收写日志流程

在消费者端，完成索引的声明，id的获取以及对submit做JSON的序列化

```java
@Component
@Slf4j
public class SmsWriteLogListener {

    @Autowired
    private SearchService searchService;

    private final String INDEX = "sms_submit_log_";


    @RabbitListener(queues = RabbitMQConstants.SMS_WRITE_LOG)
    public void consume(StandardSubmit submit, Channel channel, Message message) throws IOException {
        //1、调用搜索模块的添加方法，完成添加操作
        log.info("接收到存储日志的信息   submit = {}",submit);
        searchService.index(INDEX + getYear(),submit.getSequenceId().toString(), JsonUtil.obj2JSON(submit));

        //2、手动ack
        channel.basicAck(message.getMessageProperties().getDeliveryTag(),false);
    }


    public String getYear(){
        return LocalDateTime.now().getYear() + "";
    }

}
```

在使用Jackson时，发现默认情况下，不支持LocalDateTime，需要额外导入依赖并且添加注解

```xml
<dependency>
    <groupId>com.fasterxml.jackson.datatype</groupId>
    <artifactId>jackson-datatype-jsr310</artifactId>
    <version>2.12.5</version>
</dependency>
```

```java
/**
 * 短信的发送时间，当前系统时间
 */
@JsonSerialize(using = LocalDateTimeSerializer.class)
@JsonDeserialize(using = LocalDateTimeDeserializer.class)
private LocalDateTime sendTime;
```

## 八、修改日志操作（！！完成短信网关模块再来！！）

### 8.1 修改写日志的错误信息

发现，经过前面的测试，发生了一些问题，因为换了环境，需要额外追加一个IP白名单的信息，但是出现下述问题：

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/0a68012f3cb3404da61363b0b8703ce8.png)

ES本质是支持同时写入多个IP的，需要按照如下方式写入ES才可以

```java
["192.168.11.1","127.0.0.1"]
```

咱们现在写入的是这种

```java
"192.168.11.1,127.0.0.1"
```

需要修改一个StandardSubmit的类型，但是成本有点高，需要多处修改。

第一个事情，就是将之前测试时，写入的ip是一个字符串![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/fbf4d1fad01e4856a72efc65c2b18fc9.png)

这样的话，会造成咱们使用ip时，无法反序列化为List集合。从根源入手，之前的数据都是通多test测试方式写入到的Redis缓存，直接修改ipAddress的get方法，让序列化存储到Redis之前，就根据List集合的方式做序列化。

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/604d01bd312d4bcc9bee93707d4cd7c5.png)

搞定之后，重新写入。写入后效果如下

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/9fbc5efdc8e5452697047be1929bc301.png)

设计到common的服务，都重启一波，然后跑一下流程，可以正常写入

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/25bea571bb94486b874d79d00919a20e.png)

在IP白名单的校验的位置，之前的判断方式是这种

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/12425b2a59f0478996bc3fc8a16e684a.png)

这种方式的判断，存在问题，如果IP不为null，需要走后面的判断查看是否是白名单，修改为如下效果![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/9b513b9fca594f98aab151e22f94884a.png)

### 8.2 修改日志操作

首先咱们已经在短信网关模块中，让修改日志的消息，通过死信队列延迟10s了。

But，但是在一些比较极端的情况下，依然可能会出现修改操作早于写日志的操作，这样会导致ES操作时，无法找到需要修改的文档信息，导致报错。

1、需要优先通过ES查询当前日志信息是否存在于ES中。

2、如果日志存在，直接执行修改日志的操作即可。

3、如果日志不存在，可以再次投递消息到死信队列中，再次延迟10s，如果还没有完成写操作，直接记录error日志。

#### 8.2.1 接收修改日志的消息

直接在搜索模块的mq包下，声明接收修改日志消息的信息

```java
@Component
@Slf4j
public class SmsUpdateLogListener {

    @RabbitListener(queues = {RabbitMQConstants.SMS_GATEWAY_DEAD_QUEUE})
    public void consume(StandardReport report, Channel channel, Message message){
        log.info("【搜素模块-修改日志】 接收到修改日志的消息 report = {}", report);
    }
}

```

日志信息：

![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/2746/1680868630093/364580f09a98455b85cbc79a0b0e8947.png)

#### 8.2.2 查看日志是否存在

基于Elasticsearch的RestHighLevelClient实现查询指定索引下的文档是否存在

```java
@Override
public boolean exists(String index, String id) throws IOException {
    // 构建GetRequest，查看索引是否存在
    GetRequest request = new GetRequest();

    // 指定索引信息，还有文档id
    request.index(index);
    request.id(id);

    // 基于restHighLevelClient将查询指定id的文档是否存在的请求投递过去。
    boolean exists = restHighLevelClient.exists(request, RequestOptions.DEFAULT);

    // 直接返回信息
    return exists;
}
```

#### 8.2.3 完成修改日志操作

直接在消费消息的位置，调用SearchService，实现修改日志信息

1、为了可以在ElasticsearchServiceImpl类中获取到消费者中的report中，基于ThreadLocal做传参。

2、优先在业务中判断是否存在

* 第一次不存在，重新投递消息，再次延迟10s。
* 第二次不存在，直接记录error日志信息，后期再处理

3、如果日志存在，没问题，直接修改Elasticsearch的文档

消费者执行编写好的Service功能，同时确认消息投递到了ThreadLocal

```java
@Component
@Slf4j
public class SmsUpdateLogListener {


    @Autowired
    private SearchService searchService;

    @RabbitListener(queues = {RabbitMQConstants.SMS_GATEWAY_DEAD_QUEUE})
    public void consume(StandardReport report, Channel channel, Message message) throws IOException {
        log.info("【搜素模块-修改日志】 接收到修改日志的消息 report = {}", report);
        // 将report对象存储ThreadLocal中，方便在搜索模块中获取
        SearchUtils.set(report);
        // 调用搜索模块完成的修改操作
        Map<String,Object> doc = new HashMap<>();
        doc.put("reportState",report.getReportState());
        searchService.update(SearchUtils.INDEX + SearchUtils.getYear(),report.getSequenceId().toString(),doc);

        // ack
        channel.basicAck(message.getMessageProperties().getDeliveryTag(),false);
    }
}
```

Service层处理具体的业务

```java
@Override
public void update(String index, String id, Map<String, Object> doc) throws IOException {
    //1、基于exists方法，查询当前文档是否存在
    boolean exists = exists(index, id);
    if(!exists){
        // 当前文档不存在
        StandardReport report = SearchUtils.get();
        if(report.getReUpdate()){
            // 第二次获取投递的消息，到这已经是延迟20s了。
            log.error("【搜索模块-修改日志】 修改日志失败，report = {}",report);
        }else{
            // 第一次投递，可以再次将消息仍会MQ中
            // 开始第二次消息的投递了
            report.setReUpdate(true);
            rabbitTemplate.convertAndSend(RabbitMQConstants.SMS_GATEWAY_NORMAL_QUEUE,report);
        }
        SearchUtils.remove();
        return;
    }
    //2、到这，可以确认文档是存在的，直接做修改操作
    UpdateRequest request = new UpdateRequest();

    request.index(index);
    request.id(id);
    request.doc(doc);

    UpdateResponse update = restHighLevelClient.update(request, RequestOptions.DEFAULT);
    String result = update.getResult().getLowercase();
    if(!UPDATED.equals(result)){
        // 添加失败！！
        log.error("【搜索模块-修改日志失败】 index = {},id = {},doc = {}",index,id,doc);
        throw new SearchException(ExceptionEnums.SEARCH_UPDATE_ERROR);
    }
    log.info("【搜索模块-修改日志成功】 文档修改成功index = {},id = {},doc = {}",index,id,doc);

}
```
